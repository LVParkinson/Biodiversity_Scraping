{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Observer\n",
    "\n",
    "## A project to collect the thousands of observations of the natural world from Reddit (and maybe eventually other social media). Photos, identification, and any location information are collated to create a usable dataset for citizen science networks such as eBird and iNaturalist. We hope... eventually...\n",
    "\n",
    "### Authors: Lindsey Parkinson, Thomas Oliver, and Roman Grisch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Reddit API PRAW. You must have a Reddit account in order to use the notebook. I have my information saved in a seperate json file for anonymity. You can add your own credentials below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.1.0 of praw is outdated. Version 7.3.0 was released Thursday June 17, 2021.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#redditkeys.json contains all the information necessary to use the Reddit API\n",
    "working_directory = os.getcwd()\n",
    "file_path = working_directory + '/redditkeys.json'\n",
    "\n",
    "with open(file_path) as infile:\n",
    "    credentials = json.load(infile)\n",
    "reddit = praw.Reddit(client_id = credentials[\"client_id\"],\n",
    "                     client_secret = credentials[\"client_secret\"],\n",
    "                     user_agent=credentials[\"user_agent\"],\n",
    "                     username=credentials[\"username\"],\n",
    "                     password=credentials[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to ensure it is associated with your Reddit account:\n",
    "#print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many r/whatisthis___ or r/whatsthis___ subreddits used for plant, animal, and fungus identification. Here we use r/whatsthisfish as an example though multiple subreddits can be added to subreddit_list below. \n",
    "\n",
    "Other subreddits include:\n",
    "r/whatisthisfish,\n",
    "r/whatsthisbug,\n",
    "r/whatsthisbird,\n",
    "r/whatsthissnake\n",
    "\n",
    "The subreddits above follow a standard protocol enforced by the moderators making the scraping of novel observations easier. However, the following subreddits may also be worth considering:\n",
    "r/slimemolds,\n",
    "r/whatsthisplant,\n",
    "r/animalid,\n",
    "r/PlantIdentification,\n",
    "r/treeidentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whatsthisfish completed; total 100 posts scraped\n"
     ]
    }
   ],
   "source": [
    "date_list = []\n",
    "#author_list = []\n",
    "id_list = []\n",
    "link_flair_text_list = []\n",
    "title_list = []\n",
    "url_list = []\n",
    "top_comment_list = []\n",
    "\n",
    "\n",
    "#subreddits we want to scrape information from\n",
    "subreddit_list= ['whatsthisfish']\n",
    "\n",
    "#What information we want from each subreddit post\n",
    "for subred in subreddit_list:\n",
    "    subreddit = reddit.subreddit(subred)\n",
    "    top_post = subreddit.top(limit = 100)  #how many posts from the subreddit we want to pull\n",
    "    \n",
    "    for sub in top_post:        \n",
    "        date_list.append(datetime.datetime.fromtimestamp(sub.created_utc))\n",
    "        #author_list.append(sub.author)\n",
    "        id_list.append(sub.id)        \n",
    "        link_flair_text_list.append(sub.link_flair_text)\n",
    "        title_list.append(sub.title)\n",
    "        url_list.append(sub.url)\n",
    "        \n",
    "    print(subred, 'completed; ', end='')\n",
    "    print('total', len(title_list), 'posts scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date': date_list,\n",
    "                   'ID':id_list, \n",
    "                   #'Author':author_list, \n",
    "                   'Title':title_list,\n",
    "                   'Flair':link_flair_text_list,\n",
    "                   'URL':url_list\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(row, col = \"URL\"):\n",
    "    \"\"\"\n",
    "    This function will convert strings into hyperlinks readable when exported into csv or pdf. \n",
    "    Should make it easier to pull images\n",
    "    \"\"\"\n",
    "    return \"<a href='{}'>{}</a>\".format(row[col], row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['URL'] = df.apply(convert, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting top comment\n",
    "This code extracts the comment tree of the first comment block then the first comment of the block. Our hope was that this comment will contain the correct identification because participants are supposed to upvote the answers they agree with. \n",
    "\n",
    "I added the if/else statement below to try and deal with posts that don't seem to have comments. Honestly, It doesn't work. Some subreddits I scrape the comment column ends up 1 or 2 rows shorter and I haven't figured out why.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = defaultdict(list)\n",
    "            \n",
    "for ID in id_list:\n",
    "    submission = reddit.submission(str(ID))\n",
    "    for top_level_comment in submission.comments:\n",
    "        if top_level_comment is not None:\n",
    "            comments[submission.title].append(top_level_comment.body)\n",
    "        else:\n",
    "            comments[submission.title].append(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_comment = []\n",
    "    \n",
    "for key, val in comments.items():\n",
    "    if val is not None:\n",
    "        top_comment.append(val[0])\n",
    "    else:\n",
    "        top_comment.apend(\"NA\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"Top Comment\"] = top_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting location\n",
    "The location of the observation should be written within the title of the post. In the following code chunks we use the nltk package to tokenize the post titles and attempt to extract location words. \n",
    "\n",
    "If this is your first time using nltk or this notebook you may need to remove the # and download the packages below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and download NLP tools\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A function to pull location information from sentence chunks\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = df[\"Title\"].tolist()\n",
    "location = []\n",
    "\n",
    "for item in titles_list:\n",
    "    sentences = nltk.sent_tokenize(item)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    \n",
    "    entities = []  \n",
    "    for tree in chunked_sentences:\n",
    "        entities.extend(extract_entity_names(tree))\n",
    "    location.append(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Flair</th>\n",
       "      <th>URL</th>\n",
       "      <th>Top Comment</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-26 04:40:05</td>\n",
       "      <td>ji7jvh</td>\n",
       "      <td>Anybody know what species this is? Found in Ja...</td>\n",
       "      <td>Identified, probably</td>\n",
       "      <td>&lt;a href='https://i.redd.it/bybemwp61dv51.jpg'&gt;...</td>\n",
       "      <td>Florida pompano,  *Trachinotus carolinus* . Th...</td>\n",
       "      <td>[Anybody, Jacksonville, Inshore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-21 17:49:37</td>\n",
       "      <td>l2299m</td>\n",
       "      <td>Picture taken in Central Florida. Freshwater p...</td>\n",
       "      <td>Family known, species unidentified</td>\n",
       "      <td>&lt;a href='https://i.redd.it/ow7f7ozctpc61.jpg'&gt;...</td>\n",
       "      <td>Common pleco. Non-native species in Florida.</td>\n",
       "      <td>[Picture, Central Florida, Freshwater]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-13 21:57:12</td>\n",
       "      <td>nbptao</td>\n",
       "      <td>Off-shore Palm Beach at around 70ft. Photograp...</td>\n",
       "      <td>Identified, probably</td>\n",
       "      <td>&lt;a href='https://i.redd.it/1crpkkcx0yy61.jpg'&gt;...</td>\n",
       "      <td>It would appear to be a [juvenile louvar](http...</td>\n",
       "      <td>[Palm Beach, Photographer, Michael Patrick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-15 00:58:58</td>\n",
       "      <td>h93riw</td>\n",
       "      <td>Caught in key west, Florida. Never seen anythi...</td>\n",
       "      <td>Identified, high confidence</td>\n",
       "      <td>&lt;a href='https://i.redd.it/j86d87vnhy451.jpg'&gt;...</td>\n",
       "      <td>Took me awhile, but its a swallow-tailed bass ...</td>\n",
       "      <td>[Caught, Florida]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-11 17:59:31</td>\n",
       "      <td>ghq9mn</td>\n",
       "      <td>I found this video and the fish is hella cute ...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;a href='https://v.redd.it/lvximb7tr5y41'&gt;4&lt;/a&gt;</td>\n",
       "      <td>Looks like spotted porcupinefish also known as...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-02-20 23:35:57</td>\n",
       "      <td>loiigf</td>\n",
       "      <td>Found: French Beach, Vancouver Island</td>\n",
       "      <td>Family known, species maybe IDed</td>\n",
       "      <td>&lt;a href='https://i.redd.it/x53ciq2impi61.jpg'&gt;...</td>\n",
       "      <td>Some kind of sculpin I believe.\\n\\nI misread t...</td>\n",
       "      <td>[French Beach, Vancouver Island]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2020-12-27 23:06:35</td>\n",
       "      <td>klclk8</td>\n",
       "      <td>Twirling fish in the FL Keys</td>\n",
       "      <td>Identified, high confidence</td>\n",
       "      <td>&lt;a href='https://v.redd.it/ivfqdbqzys761'&gt;96&lt;/a&gt;</td>\n",
       "      <td>Looks like a [Tripletail](https://www.fishbase...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020-12-20 15:22:49</td>\n",
       "      <td>kguscz</td>\n",
       "      <td>Found this tiny fish stranded on the ice in ea...</td>\n",
       "      <td>Family known, species maybe IDed</td>\n",
       "      <td>&lt;a href='https://i.redd.it/ww2sedbzpc661.jpg'&gt;...</td>\n",
       "      <td>Looks like a kind of sculpin (family Cottidae)...</td>\n",
       "      <td>[Norway]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2020-12-16 18:00:12</td>\n",
       "      <td>keczcj</td>\n",
       "      <td>I found this tiny blue thing dead on the subst...</td>\n",
       "      <td>Identified, probably</td>\n",
       "      <td>&lt;a href='https://i.redd.it/9q66hwf9xk561.jpg'&gt;...</td>\n",
       "      <td>I think its some kind of insect larvae. Maybe ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2020-12-04 23:45:30</td>\n",
       "      <td>k6vgb4</td>\n",
       "      <td>What species is this fish?</td>\n",
       "      <td>Identified, high confidence</td>\n",
       "      <td>&lt;a href='https://i.redd.it/d674xxg319361.jpg'&gt;...</td>\n",
       "      <td>synodontis eupterus</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date      ID  \\\n",
       "0  2020-10-26 04:40:05  ji7jvh   \n",
       "1  2021-01-21 17:49:37  l2299m   \n",
       "2  2021-05-13 21:57:12  nbptao   \n",
       "3  2020-06-15 00:58:58  h93riw   \n",
       "4  2020-05-11 17:59:31  ghq9mn   \n",
       "..                 ...     ...   \n",
       "95 2021-02-20 23:35:57  loiigf   \n",
       "96 2020-12-27 23:06:35  klclk8   \n",
       "97 2020-12-20 15:22:49  kguscz   \n",
       "98 2020-12-16 18:00:12  keczcj   \n",
       "99 2020-12-04 23:45:30  k6vgb4   \n",
       "\n",
       "                                                Title  \\\n",
       "0   Anybody know what species this is? Found in Ja...   \n",
       "1   Picture taken in Central Florida. Freshwater p...   \n",
       "2   Off-shore Palm Beach at around 70ft. Photograp...   \n",
       "3   Caught in key west, Florida. Never seen anythi...   \n",
       "4   I found this video and the fish is hella cute ...   \n",
       "..                                                ...   \n",
       "95              Found: French Beach, Vancouver Island   \n",
       "96                       Twirling fish in the FL Keys   \n",
       "97  Found this tiny fish stranded on the ice in ea...   \n",
       "98  I found this tiny blue thing dead on the subst...   \n",
       "99                         What species is this fish?   \n",
       "\n",
       "                                 Flair  \\\n",
       "0                 Identified, probably   \n",
       "1   Family known, species unidentified   \n",
       "2                 Identified, probably   \n",
       "3          Identified, high confidence   \n",
       "4                                 None   \n",
       "..                                 ...   \n",
       "95    Family known, species maybe IDed   \n",
       "96         Identified, high confidence   \n",
       "97    Family known, species maybe IDed   \n",
       "98                Identified, probably   \n",
       "99         Identified, high confidence   \n",
       "\n",
       "                                                  URL  \\\n",
       "0   <a href='https://i.redd.it/bybemwp61dv51.jpg'>...   \n",
       "1   <a href='https://i.redd.it/ow7f7ozctpc61.jpg'>...   \n",
       "2   <a href='https://i.redd.it/1crpkkcx0yy61.jpg'>...   \n",
       "3   <a href='https://i.redd.it/j86d87vnhy451.jpg'>...   \n",
       "4     <a href='https://v.redd.it/lvximb7tr5y41'>4</a>   \n",
       "..                                                ...   \n",
       "95  <a href='https://i.redd.it/x53ciq2impi61.jpg'>...   \n",
       "96   <a href='https://v.redd.it/ivfqdbqzys761'>96</a>   \n",
       "97  <a href='https://i.redd.it/ww2sedbzpc661.jpg'>...   \n",
       "98  <a href='https://i.redd.it/9q66hwf9xk561.jpg'>...   \n",
       "99  <a href='https://i.redd.it/d674xxg319361.jpg'>...   \n",
       "\n",
       "                                          Top Comment  \\\n",
       "0   Florida pompano,  *Trachinotus carolinus* . Th...   \n",
       "1        Common pleco. Non-native species in Florida.   \n",
       "2   It would appear to be a [juvenile louvar](http...   \n",
       "3   Took me awhile, but its a swallow-tailed bass ...   \n",
       "4   Looks like spotted porcupinefish also known as...   \n",
       "..                                                ...   \n",
       "95  Some kind of sculpin I believe.\\n\\nI misread t...   \n",
       "96  Looks like a [Tripletail](https://www.fishbase...   \n",
       "97  Looks like a kind of sculpin (family Cottidae)...   \n",
       "98  I think its some kind of insect larvae. Maybe ...   \n",
       "99                                synodontis eupterus   \n",
       "\n",
       "                                       Location  \n",
       "0              [Anybody, Jacksonville, Inshore]  \n",
       "1        [Picture, Central Florida, Freshwater]  \n",
       "2   [Palm Beach, Photographer, Michael Patrick]  \n",
       "3                             [Caught, Florida]  \n",
       "4                                            []  \n",
       "..                                          ...  \n",
       "95             [French Beach, Vancouver Island]  \n",
       "96                                           []  \n",
       "97                                     [Norway]  \n",
       "98                                           []  \n",
       "99                                           []  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Location\"] = location\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/lindsey/anaconda3/lib/python3.8/site-packages/geograpy/locs.db.gz from http://wiki.bitplan.com/images/confident/locs.db.gz ... this might take a few seconds\n",
      "unzipping /home/lindsey/anaconda3/lib/python3.8/site-packages/geograpy/locs.db from /home/lindsey/anaconda3/lib/python3.8/site-packages/geograpy/locs.db.gz\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Jacksonville']\n",
      "other=['Anybody']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Picture']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'Puerto Rico', 'United States']\n",
      "regions=[]\n",
      "cities=['Florida']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'Puerto Rico', 'United States']\n",
      "regions=[]\n",
      "cities=['Florida']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Singapore']\n",
      "regions=['Singapore']\n",
      "cities=['Singapore']\n",
      "other=[]\n",
      "countries=['France']\n",
      "regions=[]\n",
      "cities=['Came']\n",
      "other=['Brunei']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Help']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Etowah']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United Kingdom']\n",
      "regions=[]\n",
      "cities=['Looe']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Canada', 'Spain', 'United States']\n",
      "regions=['Canada', 'Ontario']\n",
      "cities=['Canada', 'Ontario']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Greece']\n",
      "regions=['Mytilene', 'Greece']\n",
      "cities=['Mytilene']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Chinook']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Seems']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'Puerto Rico', 'United States']\n",
      "regions=[]\n",
      "cities=['Florida']\n",
      "other=['Caught']\n",
      "countries=['Australia', 'South Africa', 'United States']\n",
      "regions=[]\n",
      "cities=['Vermont']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['South']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Boca Raton']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Islamorada']\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Louisiana']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Found']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Petsmart']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'Puerto Rico', 'United States']\n",
      "regions=[]\n",
      "cities=['Florida']\n",
      "other=['Caught']\n",
      "countries=['Hong Kong']\n",
      "regions=['Hong Kong']\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Crosspost', 'Instagram', 'French']\n",
      "countries=['Greenland', 'United States']\n",
      "regions=['Greenland']\n",
      "cities=['Greenland']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'Puerto Rico', 'United States']\n",
      "regions=[]\n",
      "cities=['Florida']\n",
      "other=[]\n",
      "countries=['Portugal']\n",
      "regions=['Portugal']\n",
      "cities=['Portugal']\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['Tampa']\n",
      "other=['Caught']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['South Florida']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Hi']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Italy']\n",
      "regions=[]\n",
      "cities=['Front']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Caught']\n",
      "countries=['France']\n",
      "regions=['France']\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['United Kingdom', 'United States', 'Italy']\n",
      "regions=[]\n",
      "cities=['Washington']\n",
      "other=['Puget Sound']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Brazil', 'Costa Rica', 'United States']\n",
      "regions=[]\n",
      "cities=['California']\n",
      "other=['Caught']\n",
      "countries=['Mexico', 'Philippines', 'Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'United States', 'Puerto Rico']\n",
      "regions=['Florida', 'Mexico']\n",
      "cities=['Mexico', 'Florida']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['China', 'Mexico', 'United States']\n",
      "regions=['China']\n",
      "cities=['China']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Stuff']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Fish']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['South Africa']\n",
      "regions=['Cape Town', 'South Africa']\n",
      "cities=['Cape Town']\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=['US']\n",
      "cities=[]\n",
      "other=['US']\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['North']\n",
      "other=['North']\n",
      "countries=['Japan']\n",
      "regions=[]\n",
      "cities=['Tokyo']\n",
      "other=[]\n",
      "countries=['Hungary', 'United States']\n",
      "regions=[]\n",
      "cities=['Nova']\n",
      "other=[]\n",
      "countries=['United States']\n",
      "regions=[]\n",
      "cities=['North']\n",
      "other=['North']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Serbia', 'Belgium', 'United States']\n",
      "regions=['Serbia', 'Belgrade']\n",
      "cities=['Belgrade']\n",
      "other=['Serbia']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Hi']\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'United States', 'Puerto Rico']\n",
      "regions=[]\n",
      "cities=['Florida', 'Oskaloosa']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Australia', 'Canada']\n",
      "regions=['Caught', 'Sydney', 'Australia']\n",
      "cities=['Sydney']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Saw']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=['Aquarium']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Mexico', 'Philippines', 'Argentina', 'Uruguay', 'El Salvador', 'Honduras', 'Bolivia, Plurinational State of', 'United States']\n",
      "regions=['Mexico', 'La Paz', 'Hi']\n",
      "cities=['Mexico', 'La Paz']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Brazil', 'Costa Rica', 'United States']\n",
      "regions=[]\n",
      "cities=['California']\n",
      "other=['Took']\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Argentina', 'Uruguay', 'Brazil', 'Trinidad and Tobago', 'Honduras', 'Costa Rica', 'Colombia', 'United States', 'Puerto Rico']\n",
      "regions=[]\n",
      "cities=['Florida', 'West Palm Beach']\n",
      "other=[]\n",
      "countries=['New Zealand']\n",
      "regions=['New Zealand']\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=['Norway', 'United States']\n",
      "regions=['Norway']\n",
      "cities=['Norway']\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n",
      "countries=[]\n",
      "regions=[]\n",
      "cities=[]\n",
      "other=[]\n"
     ]
    }
   ],
   "source": [
    "import geograpy\n",
    "#url='https://en.wikipedia.org/wiki/2012_Summer_Olympics_torch_relay'\n",
    "for title in df['Title']:\n",
    "    places = geograpy.get_geoPlace_context(text = title)\n",
    "    print(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anybody', 'Jacksonville']\n",
      "['Picture']\n",
      "[]\n",
      "['Caught', 'Florida']\n",
      "[]\n",
      "[]\n",
      "['Florida']\n",
      "[]\n",
      "[]\n",
      "['Singapore']\n",
      "['Came', 'Brunei']\n",
      "['Help']\n",
      "[]\n",
      "['Etowah']\n",
      "[]\n",
      "['Caught', 'Looe']\n",
      "[]\n",
      "['Ontario', 'Canada']\n",
      "[]\n",
      "['Mytilene', 'Greece']\n",
      "[]\n",
      "['Caught', 'Chinook']\n",
      "['Seems']\n",
      "[]\n",
      "['Caught', 'Florida']\n",
      "['Caught', 'Vermont']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['South']\n",
      "[]\n",
      "['Caught', 'Boca Raton']\n",
      "[]\n",
      "['Islamorada']\n",
      "['Louisiana']\n",
      "[]\n",
      "['Found']\n",
      "[]\n",
      "['Caught']\n",
      "['Petsmart']\n",
      "[]\n",
      "['Caught', 'Florida']\n",
      "['Hong Kong']\n",
      "['Crosspost', 'Instagram', 'French']\n",
      "['Greenland']\n",
      "[]\n",
      "['Florida']\n",
      "['Portugal']\n",
      "['Caught', 'Tampa']\n",
      "[]\n",
      "[]\n",
      "['South Florida']\n",
      "['Hi']\n",
      "[]\n",
      "['Front']\n",
      "[]\n",
      "[]\n",
      "['Caught']\n",
      "['France']\n",
      "['Puget Sound', 'Washington']\n",
      "[]\n",
      "['Caught', 'California']\n",
      "['Florida', 'Mexico']\n",
      "[]\n",
      "[]\n",
      "['China']\n",
      "[]\n",
      "[]\n",
      "['Stuff']\n",
      "['Fish']\n",
      "[]\n",
      "['Cape Town', 'South Africa']\n",
      "['US']\n",
      "['North']\n",
      "['Tokyo']\n",
      "['Nova']\n",
      "['North']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Belgrade', 'Serbia']\n",
      "['Hi']\n",
      "['Oskaloosa', 'Florida']\n",
      "[]\n",
      "['Caught', 'Sydney', 'Australia']\n",
      "['Saw']\n",
      "['Aquarium']\n",
      "[]\n",
      "[]\n",
      "['Hi', 'La Paz', 'Mexico']\n",
      "[]\n",
      "['Took', 'California']\n",
      "[]\n",
      "['West Palm Beach', 'Florida']\n",
      "['New Zealand']\n",
      "[]\n",
      "[]\n",
      "['Norway']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from geograpy import extraction\n",
    "\n",
    "for title in df['Title']:\n",
    "    e = extraction.Extractor(text = title)\n",
    "    e.find_geoEntities()\n",
    "# You can now access all of the places found by the Extractor\n",
    "    print(e.places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
